{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Energy Gamma:Hadron Separation with Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Zhixiang's Bin 0 prep-pprocessed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: training_sweets_dec19_noise_MPF_allParticle_bin_0.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdf = pd.HDFStore('training_sweets_dec19_noise_MPF_allParticle_bin_0.h5')\n",
    "print(hdf)\n",
    "df = hdf['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zenithAngle</th>\n",
       "      <th>coreFiduScale</th>\n",
       "      <th>nHit</th>\n",
       "      <th>nHitSP10</th>\n",
       "      <th>CxPE40</th>\n",
       "      <th>PINC</th>\n",
       "      <th>SFCFChi2</th>\n",
       "      <th>nHitSP20</th>\n",
       "      <th>GamCoreAge</th>\n",
       "      <th>numPoints</th>\n",
       "      <th>...</th>\n",
       "      <th>fAnnulusCharge0</th>\n",
       "      <th>fAnnulusCharge1</th>\n",
       "      <th>fAnnulusCharge2</th>\n",
       "      <th>fAnnulusCharge3</th>\n",
       "      <th>compactness</th>\n",
       "      <th>nHit10ratio</th>\n",
       "      <th>nHitRatio</th>\n",
       "      <th>signal</th>\n",
       "      <th>pclass</th>\n",
       "      <th>TWgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "      <td>767000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.409307</td>\n",
       "      <td>0.605330</td>\n",
       "      <td>0.490134</td>\n",
       "      <td>0.619248</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.078640</td>\n",
       "      <td>0.682283</td>\n",
       "      <td>0.284568</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258955</td>\n",
       "      <td>0.105532</td>\n",
       "      <td>0.060645</td>\n",
       "      <td>0.077713</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.746458</td>\n",
       "      <td>0.639234</td>\n",
       "      <td>0.303649</td>\n",
       "      <td>2.189791</td>\n",
       "      <td>115.135239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.199939</td>\n",
       "      <td>0.238346</td>\n",
       "      <td>0.284824</td>\n",
       "      <td>0.169445</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.072563</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.414085</td>\n",
       "      <td>0.185039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>0.081066</td>\n",
       "      <td>0.100594</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>0.180972</td>\n",
       "      <td>0.142209</td>\n",
       "      <td>0.459833</td>\n",
       "      <td>1.148675</td>\n",
       "      <td>330.270053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.257815</td>\n",
       "      <td>0.422819</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.041335</td>\n",
       "      <td>0.027136</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.005958</td>\n",
       "      <td>0.655882</td>\n",
       "      <td>0.557886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.375698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.392299</td>\n",
       "      <td>0.651007</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.063593</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.650539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.623364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.543271</td>\n",
       "      <td>0.778523</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.096184</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.742222</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.737108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.003465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         zenithAngle  coreFiduScale           nHit       nHitSP10  \\\n",
       "count  767000.000000  767000.000000  767000.000000  767000.000000   \n",
       "mean        0.409307       0.605330       0.490134       0.619248   \n",
       "std         0.199939       0.238346       0.284824       0.169445   \n",
       "min         0.000286       0.000000       0.000000       0.000000   \n",
       "25%         0.257815       0.422819       0.250000       0.531915   \n",
       "50%         0.392299       0.651007       0.500000       0.638298   \n",
       "75%         0.543271       0.778523       0.750000       0.744681   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              CxPE40           PINC       SFCFChi2       nHitSP20  \\\n",
       "count  767000.000000  767000.000000  767000.000000  767000.000000   \n",
       "mean        0.003864       0.074989       0.078640       0.682283   \n",
       "std         0.009478       0.048477       0.072563       0.163662   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000685       0.041335       0.027136       0.604651   \n",
       "50%         0.001775       0.063593       0.055627       0.697674   \n",
       "75%         0.003976       0.096184       0.104838       0.790698   \n",
       "max         1.000000       0.984499       1.000000       1.000000   \n",
       "\n",
       "          GamCoreAge      numPoints      ...        fAnnulusCharge0  \\\n",
       "count  767000.000000  767000.000000      ...          767000.000000   \n",
       "mean        0.284568       0.122400      ...               0.258955   \n",
       "std         0.414085       0.185039      ...               0.253437   \n",
       "min         0.000000       0.000000      ...               0.000000   \n",
       "25%         0.000000       0.000000      ...               0.000000   \n",
       "50%         0.000000       0.000000      ...               0.230000   \n",
       "75%         0.742222       0.285714      ...               0.450000   \n",
       "max         1.000000       1.000000      ...               1.000000   \n",
       "\n",
       "       fAnnulusCharge1  fAnnulusCharge2  fAnnulusCharge3    compactness  \\\n",
       "count    767000.000000    767000.000000    767000.000000  767000.000000   \n",
       "mean          0.105532         0.060645         0.077713       0.027645   \n",
       "std           0.123061         0.081066         0.100594       0.035675   \n",
       "min           0.000000         0.000000         0.000000       0.000000   \n",
       "25%           0.000000         0.000000         0.010101       0.005958   \n",
       "50%           0.070000         0.030612         0.040404       0.013792   \n",
       "75%           0.170000         0.091837         0.111111       0.036920   \n",
       "max           1.000000         1.000000         1.000000       1.000000   \n",
       "\n",
       "         nHit10ratio      nHitRatio         signal         pclass  \\\n",
       "count  767000.000000  767000.000000  767000.000000  767000.000000   \n",
       "mean        0.746458       0.639234       0.303649       2.189791   \n",
       "std         0.180972       0.142209       0.459833       1.148675   \n",
       "min         0.000000       0.012887       0.000000       0.000000   \n",
       "25%         0.655882       0.557886       0.000000       1.000000   \n",
       "50%         0.798319       0.650539       0.000000       3.000000   \n",
       "75%         0.882353       0.737108       1.000000       3.000000   \n",
       "max         1.000000       1.000000       1.000000       3.000000   \n",
       "\n",
       "                TWgt  \n",
       "count  767000.000000  \n",
       "mean      115.135239  \n",
       "std       330.270053  \n",
       "min         0.000005  \n",
       "25%         7.375698  \n",
       "50%        36.623364  \n",
       "75%       111.003465  \n",
       "max     10000.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zenithAngle</th>\n",
       "      <th>coreFiduScale</th>\n",
       "      <th>nHit</th>\n",
       "      <th>nHitSP10</th>\n",
       "      <th>CxPE40</th>\n",
       "      <th>PINC</th>\n",
       "      <th>SFCFChi2</th>\n",
       "      <th>nHitSP20</th>\n",
       "      <th>GamCoreAge</th>\n",
       "      <th>numPoints</th>\n",
       "      <th>...</th>\n",
       "      <th>fAnnulusCharge0</th>\n",
       "      <th>fAnnulusCharge1</th>\n",
       "      <th>fAnnulusCharge2</th>\n",
       "      <th>fAnnulusCharge3</th>\n",
       "      <th>compactness</th>\n",
       "      <th>nHit10ratio</th>\n",
       "      <th>nHitRatio</th>\n",
       "      <th>signal</th>\n",
       "      <th>pclass</th>\n",
       "      <th>TWgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2654828</th>\n",
       "      <td>0.600648</td>\n",
       "      <td>0.651007</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.085851</td>\n",
       "      <td>0.067106</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>0.783914</td>\n",
       "      <td>0.662060</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.606366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6273597</th>\n",
       "      <td>0.425276</td>\n",
       "      <td>0.570470</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.038951</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.033573</td>\n",
       "      <td>0.832817</td>\n",
       "      <td>0.455206</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.222332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137205</th>\n",
       "      <td>0.133626</td>\n",
       "      <td>0.308725</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.115660</td>\n",
       "      <td>0.105005</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.844291</td>\n",
       "      <td>0.579730</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>52.696308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080706</th>\n",
       "      <td>0.543366</td>\n",
       "      <td>0.583893</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.027822</td>\n",
       "      <td>0.079396</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.219814</td>\n",
       "      <td>0.405768</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9.732318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514925</th>\n",
       "      <td>0.146969</td>\n",
       "      <td>0.919463</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.459168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>226.415630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         zenithAngle  coreFiduScale    nHit  nHitSP10    CxPE40      PINC  \\\n",
       "2654828     0.600648       0.651007  1.0000  0.787234  0.003072  0.085851   \n",
       "6273597     0.425276       0.570470  0.3125  0.638298  0.000762  0.038951   \n",
       "137205      0.133626       0.308725  0.0625  0.574468  0.004341  0.115660   \n",
       "3080706     0.543366       0.583893  0.3125  0.170213  0.001240  0.027822   \n",
       "1514925     0.146969       0.919463  0.4375  0.659574  0.000709  0.023847   \n",
       "\n",
       "         SFCFChi2  nHitSP20  GamCoreAge  numPoints     ...      \\\n",
       "2654828  0.067106  0.813953         1.0   0.476190     ...       \n",
       "6273597  0.018246  0.697674         1.0   0.428571     ...       \n",
       "137205   0.105005  0.558140         0.0   0.000000     ...       \n",
       "3080706  0.079396  0.348837         0.0   0.000000     ...       \n",
       "1514925  0.014036  0.697674         0.0   0.000000     ...       \n",
       "\n",
       "         fAnnulusCharge0  fAnnulusCharge1  fAnnulusCharge2  fAnnulusCharge3  \\\n",
       "2654828             0.28             0.12         0.102041         0.020202   \n",
       "6273597             0.33             0.22         0.061224         0.050505   \n",
       "137205              0.44             0.00         0.010204         0.030303   \n",
       "3080706             0.66             0.00         0.000000         0.000000   \n",
       "1514925             0.00             0.00         0.000000         0.000000   \n",
       "\n",
       "         compactness  nHit10ratio  nHitRatio  signal  pclass        TWgt  \n",
       "2654828     0.009490     0.783914   0.662060       0       2   33.606366  \n",
       "6273597     0.033573     0.832817   0.455206       0       2    3.222332  \n",
       "137205      0.004914     0.844291   0.579730       0       3   52.696308  \n",
       "3080706     0.012040     0.219814   0.405768       0       3    9.732318  \n",
       "1514925     0.036092     0.814706   0.459168       1       1  226.415630  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['zenithAngle', 'coreFiduScale', 'nHit', 'nHitSP10', 'CxPE40', 'PINC',\n",
      "       'SFCFChi2', 'nHitSP20', 'GamCoreAge', 'numPoints', 'scandelCore',\n",
      "       'numSum', 'scanedFrac', 'fixedFrac', 'avePE', 'disMax', 'CxPE40SPTime',\n",
      "       'fAnnulusCharge0', 'fAnnulusCharge1', 'fAnnulusCharge2',\n",
      "       'fAnnulusCharge3', 'compactness', 'nHit10ratio', 'nHitRatio', 'signal',\n",
      "       'pclass', 'TWgt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# all features:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose 14 features for now.    \n",
    "(In one test run with 11 features, I got quite similar results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zenithAngle' 'coreFiduScale' 'nHit' 'nHitSP10' 'CxPE40' 'PINC'\n",
      " 'SFCFChi2' 'nHitSP20' 'GamCoreAge' 'numPoints' 'scandelCore' 'scanedFrac'\n",
      " 'avePE' 'compactness']\n"
     ]
    }
   ],
   "source": [
    "features = df.columns[[0,1,2,3,4,5,6,7,8,9,10,12,14,21]].values\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's separate featurs (x) and classes and weights (y).    \n",
    "All x and y data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60064811 0.65100671 1.         ... 0.70833333 0.         0.0094896 ]\n",
      " [0.4252764  0.5704698  0.3125     ... 0.47222222 0.03030303 0.03357281]\n",
      " [0.13362562 0.30872483 0.0625     ... 0.25       0.29292929 0.00491418]\n",
      " ...\n",
      " [0.24513915 0.99328859 0.75       ... 0.34722222 0.         0.01295097]\n",
      " [0.10264964 0.94630872 0.0625     ... 0.16666667 0.         0.00265152]\n",
      " [0.2926992  0.82550336 0.4375     ... 0.54166667 0.         0.03015378]]\n"
     ]
    }
   ],
   "source": [
    "x_all = df[features].values\n",
    "yw_all = df[[\"pclass\",\"TWgt\",\"signal\"]].values\n",
    "y_all = yw_all[:,0]\n",
    "w_all = yw_all[:,1]\n",
    "print(x_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling -1 to 1\n",
    "I learned that for some ML techniques (e.g. SVM), it might be better to have variables in the range -1 to 1 instead of 0 to 1 (which Zhixiang provides). So here is a quick rescaling, although I haven't really used it much below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95699469  0.19163962  1.79011193 ...  2.85635744 -0.49602369\n",
      "  -0.50891344]\n",
      " [ 0.07986919 -0.14625913 -0.6236628  ...  1.05238806 -0.27982201\n",
      "   0.16615895]\n",
      " [-1.37882864 -1.24443005 -1.50139907 ... -0.64546547  1.59392589\n",
      "  -0.63716621]\n",
      " ...\n",
      " [-0.82109123  1.62770929  0.91237567 ...  0.09734545 -0.49602369\n",
      "  -0.41188837]\n",
      " [-1.5337557   1.43060169 -1.50139907 ... -1.28216055 -0.49602369\n",
      "  -0.7005906 ]\n",
      " [-0.58321861  0.92375357 -0.18479467 ...  1.58296729 -0.49602369\n",
      "   0.07032056]]\n"
     ]
    }
   ],
   "source": [
    "#Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "sx_all = scaler.fit_transform(x_all)\n",
    "print(sx_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train/test split for a 20% test sample that is not used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test, yw_train, yw_test = train_test_split(x_all, yw_all,test_size=0.2, random_state=13)\n",
    "y_train = yw_train[:,0]\n",
    "y_test = yw_test[:,0]\n",
    "w_train = yw_train[:,1]\n",
    "w_test = yw_test[:,1]\n",
    "\n",
    "# the same for the -1 to 1 scaling:\n",
    "sx_train,sx_test, syw_train, syw_test = train_test_split(sx_all, yw_all,test_size=0.2, random_state=13)\n",
    "sy_train = syw_train[:,0]\n",
    "sy_test = syw_test[:,0]\n",
    "sw_train = syw_train[:,1]\n",
    "sw_test = syw_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For determining class weights etc., I want to store the sum over all TWgts in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21104612.14897065, 23184147.254968103, 4244005.785756631, 39775962.78694993]\n"
     ]
    }
   ],
   "source": [
    "#calculate TWgt sums:\n",
    "def sumWeights(y, classes = [0,1,2,3]):\n",
    "    tw = [y[:,1][y[:,0]==p].sum() for p in classes]\n",
    "    return tw\n",
    "print(sumWeights(yw_all))\n",
    "\n",
    "tw_all = sumWeights(yw_all)\n",
    "tw_train = sumWeights(yw_train)\n",
    "tw_test= sumWeights(yw_test)\n",
    "tw_s_train = sumWeights(syw_train)\n",
    "tw_s_test= sumWeights(syw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.738300770188633e-08, 1: 4.313292134502429e-08, 2: 2.3562644597613754e-07, 3: 2.5140811935998928e-08}\n"
     ]
    }
   ],
   "source": [
    "## class-weight:\n",
    "classes = np.arange(4)\n",
    "\n",
    "def wdict(y, classes = [0,1,2,3], relw = [1.,1.,1.,1.]):\n",
    "    # a dict with inverse sum-of-TWgt per class\n",
    "    tw = [y[:,1][y[:,0]==p].sum() for p in classes]\n",
    "    cw = {}\n",
    "    for c,p in enumerate(classes):\n",
    "        cw[c] = relw[c]/tw[c]\n",
    "    return cw\n",
    "\n",
    "cw_all = wdict(yw_all)\n",
    "print(cw_all)\n",
    "cw_train = wdict(yw_train)\n",
    "cw_test = wdict(syw_test)\n",
    "cw_s_train = wdict(syw_train)\n",
    "cw_s_test = wdict(syw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.738300770188633e-08, 1: 4.313292134502429e-08, 2: 0.00023562644597613757, 3: 2.5140811935998927e-05}\n"
     ]
    }
   ],
   "source": [
    "#1000-times weight to background:\n",
    "cw_all_k = wdict(yw_all, relw=[1.,1.,1000.,1000.])\n",
    "print(cw_all_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.738300770188633e-08, 1: 4.313292134502429e-08, 2: 0.023562644597613754, 3: 0.0025140811935998927}\n"
     ]
    }
   ],
   "source": [
    "# cw_100k : 100,000 more weight on hadron classes\n",
    "cw_100k = wdict(yw_all, relw=[1.,1.,100000.,100000.])\n",
    "print(cw_100k)\n",
    "\n",
    "#My  for now\n",
    "mycw = cw_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define functions that can provide me with precision  and recall:   \n",
    "precision: true predicted count per class divided by all predicted class associations, i.e. true+false  \n",
    "recall:    true predicted count per class divided by all true class associations   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My functions will print out various numbers, but I focus currently on two that are closest to what we have used in the past in HAWC to talk about cuts (see e.g. Crab Memo):   \n",
    "```precision hadrons:```  This is basically the rejection rate for hadrons. I think we need to be well above 90% to reject enough background, probably more like 99%    \n",
    "\n",
    "```recall gammas:``` This is the gamma passing rate. In the past, we have always aimed to have this at least at 50% for gammas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class:  [1.0, 1.0, 1.0, 1.0]\n",
      "Precision average:    1.0\n",
      "Precision gammas:     1.0\n",
      "Precision hadrons:    1.0\n"
     ]
    }
   ],
   "source": [
    "#Define precision and recall for gammas vs hadrons:\n",
    "def precision_score(yw, pred,  cw = None, classes = range(4)):\n",
    "    s = []\n",
    "    for i,c in enumerate(classes):\n",
    "        y = yw[np.round(pred) == c]\n",
    "        if cw !=None:\n",
    "            y[:,1] *= [cw[rc] for rc in y[:,0]]\n",
    "        p = np.round(pred[np.round(pred) == c])\n",
    "        s.append( y[:,1][y[:,0] - p == 0.0 ].sum()/float(y[:,1].sum()) )\n",
    "    a = np.mean(s)\n",
    "    print(\"Precision per class: \", s)\n",
    "    print(\"Precision average:   \",a)\n",
    "    #gamma precision\n",
    "    y = yw[(np.round(pred)==0) | (np.round(pred)==1)]\n",
    "    if cw !=None:\n",
    "        y[:,1] *= [cw[rc] for rc in y[:,0]]\n",
    "    p = np.round(pred[(np.round(pred)==0) | (np.round(pred)==1)])\n",
    "    ns = y[:,1][y[:,0] - p == 0.00 ].sum()/float(y[:,1].sum()) \n",
    "    print(\"Precision gammas:    \",ns)\n",
    "    #hadron precision\n",
    "    y = yw[(np.round(pred)==2) | (np.round(pred)==3)]\n",
    "    if cw !=None:\n",
    "        y[:,1] *= [cw[rc] for rc in y[:,0]]\n",
    "    p = np.round(pred[(np.round(pred)==2) | (np.round(pred)==3)])\n",
    "    ns = y[:,1][y[:,0] - p == 0.00 ].sum()/float(y[:,1].sum()) \n",
    "    print(\"Precision hadrons:   \",ns)\n",
    "#precision_score([[0,2.,0],[1,2.,0],[0,2.,0]],[0,1,0])\n",
    "precision_score(yw_all,y_all, cw = cw_all_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall per class:     [1.0, 1.0, 1.0, 1.0]\n",
      "Recall average:       1.0\n",
      "Recall gammas:        1.0\n"
     ]
    }
   ],
   "source": [
    "def recall_score(yw, pred,  cw = None, classes = range(4)):\n",
    "    s = []\n",
    "    for c in classes:\n",
    "        y = yw[yw[:,0]==c]\n",
    "        if cw !=None:\n",
    "            y[:,1] *= cw[c]\n",
    "        p = np.round(pred[yw[:,0]==c])\n",
    "        s.append( y[:,1][y[:,0] - p == 0.0 ].sum()/float(y[:,1].sum()) )\n",
    "    a = np.mean(s)\n",
    "    print(\"Recall per class:    \", s)\n",
    "    print(\"Recall average:      \",a)\n",
    "    y = yw[(yw[:,0]==0) | (yw[:,0]==1)]\n",
    "    if cw !=None:\n",
    "        y[:,1] *= [cw[rc] for rc in y[:,0]]\n",
    "    p = np.round(pred[(yw[:,0]==0) | (yw[:,0]==1)])\n",
    "    ns = y[:,1][y[:,0] - p == 0.00 ].sum()/float(y[:,1].sum()) \n",
    "    print(\"Recall gammas:       \",ns)\n",
    "#precision_score([[0,2.,0],[1,2.,0],[0,2.,0]],[0,1,0])\n",
    "recall_score(yw_all,y_all,cw=cw_all_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cw_all : equal class weights\n",
    "# [defined above]\n",
    "\n",
    "# cw_100k : 100,000 more weight on hadron classes\n",
    "cw_100k = {0: 1., 1: 1., 2: 100000., 3: 100000.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use two class weight schemes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equal class weights\n",
    "eqcw = cw_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting hadron weights much highr:\n",
    "mycw = cw_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I try a single decision tree, to get some idea about what is possible without much fuss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.metrics import precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Class Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.568546,  \n",
      "Precision per class:  [0.568011070053107, 0.6079133778216114, 0.5070052784228559, 0.3602240227110862]\n",
      "Precision average:    0.5107884372521652\n",
      "Precision gammas:     0.5860783443699025\n",
      "Precision hadrons:    0.3842217279602994\n",
      "Recall per class:     [0.5474867680850253, 0.5173637734580003, 0.1837386663860777, 0.6489958461037371]\n",
      "Recall average:       0.47439626350821007\n",
      "Recall gammas:        0.5329140700408528\n"
     ]
    }
   ],
   "source": [
    "modelDT = DecisionTreeClassifier(class_weight=eqcw)\n",
    "modelDT.fit(x_train,y_train,sample_weight=w_train)\n",
    "scoreDT = modelDT.score(x_test,y_test,sample_weight=w_test)\n",
    "print(\"accuracy: %f,  \"%scoreDT)\n",
    "pred_test = modelDT.predict(x_test)\n",
    "precision_score(yw_test,pred_test, eqcw)\n",
    "recall_score(yw_test,pred_test, eqcw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "For the 20% test sample, we get ~53% gamma recall (i.e. passing rate) and only ~38% hadron precision, i.e. rejection rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Hadron Class Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.566188,  \n",
      "Precision per class:  [1.923138170233768e-05, 1.8723769835288592e-05, 0.17853077147923385, 0.9199844506627141]\n",
      "Precision average:    0.2746382943233714\n",
      "Precision gammas:     1.8977372876778816e-05\n",
      "Precision hadrons:    0.8341075789875623\n",
      "Recall per class:     [0.5522270582228327, 0.5230940696478668, 0.15709282413781053, 0.6406739686002205]\n",
      "Recall average:       0.46827198015218263\n",
      "Recall gammas:        0.5374490208355306\n"
     ]
    }
   ],
   "source": [
    "modelDT = DecisionTreeClassifier(class_weight=mycw)\n",
    "modelDT.fit(x_train,y_train,sample_weight=w_train)\n",
    "scoreDT = modelDT.score(x_test,y_test,sample_weight=w_test)\n",
    "print(\"accuracy: %f,  \"%scoreDT)\n",
    "pred_test = modelDT.predict(x_test)\n",
    "precision_score(yw_test,pred_test, mycw)\n",
    "recall_score(yw_test,pred_test, mycw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "For the 20% test sample, we get basically the same gamma recall as before (~54%), but the hadron rejection is much better, 83% !   \n",
    "    \n",
    "So, it looks like increased class weights for the background can indeed improve hadron rejection without negatively affecting the gamma passing rate. (I should check for bugs, but it doesn't strike me as crazy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: I'll use the 100,000 time hadron weights for the following examples (i.e. dict \"mycw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, sklearn has a great feature for cross-validation. It takes a bit longer to train and I haven't actually used it here much. But it can be a great testing tool, even for NNs defined in Keras, so I left the example code in here as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\\ncvDT = cross_val_score(modelDT, x_all, y_all, cv=10)\\nprint(cvDT)\\nprint(cvDT.mean())\\npred_all = cross_val_predict(modelDT, x_all, y_all, cv=10)\\nprecision_score(yw_all,pred_all)\\nrecall_score(yw_all,pred_all)\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "cvDT = cross_val_score(modelDT, x_all, y_all, cv=10)\n",
    "print(cvDT)\n",
    "print(cvDT.mean())\n",
    "pred_all = cross_val_predict(modelDT, x_all, y_all, cv=10)\n",
    "precision_score(yw_all,pred_all)\n",
    "recall_score(yw_all,pred_all)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try an ensemble of 10 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF = RandomForestClassifier(n_estimators=10, class_weight=mycw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.662356,  \n"
     ]
    }
   ],
   "source": [
    "#Ensemble trained on training sample\n",
    "modelRF.fit(x_train,y_train,sample_weight=w_train)\n",
    "scoreRF = modelRF.score(x_test,y_test,sample_weight=w_test)\n",
    "print(\"accuracy: %f,  \"%scoreRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class:  [0.0004787284029946736, 0.0005526916680368484, 0.9942675469897593, 0.9970102729648235]\n",
      "Precision average:    0.49807731000640354\n",
      "Precision gammas:     0.0005143035312649573\n",
      "Precision hadrons:    0.9967519148889773\n",
      "Recall per class:     [0.9968626819791676, 0.9553255348012442, 0.9469446193392149, 0.981087730145052]\n",
      "Recall average:       0.9700551415661697\n",
      "Recall gammas:        0.9749516827326027\n"
     ]
    }
   ],
   "source": [
    "# a quick look at how well it worked for the training sample:\n",
    "pred_train = modelRF.predict(x_train)\n",
    "precision_score(yw_train,pred_train, mycw)\n",
    "recall_score(yw_train,pred_train, mycw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class:  [2.2257511130468307e-05, 2.8774231178356642e-05, 0.27378497080897984, 0.9185034287688303]\n",
      "Precision average:    0.29808485783002975\n",
      "Precision gammas:     2.466431317594071e-05\n",
      "Precision hadrons:    0.8967327308032879\n",
      "Recall per class:     [0.7814235342190768, 0.574653147521432, 0.07112298129138371, 0.7078257643107008]\n",
      "Recall average:       0.5337563568356483\n",
      "Recall gammas:        0.6765369209784964\n"
     ]
    }
   ],
   "source": [
    "#More importantly, here are teh test sample results:\n",
    "pred_test = modelRF.predict(x_test)\n",
    "precision_score(yw_test,pred_test, mycw)\n",
    "recall_score(yw_test,pred_test, mycw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as expected, we got a better result: hadron precision at ~90% and a very good gamma recall of ~68% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn import svm\\nmodelSVM = svm.SVC(kernel=\\'linear\\', C=1, class_weight=mycw)\\nmodelSVM.fit(sx_train,np.ascontiguousarray(sy_train),sample_weight=np.ascontiguousarray(sw_train))\\nscoreSVM = modelSVM.score(sx_test,sy_test,sample_weight=sw_test)\\nprint(\"accuracy: %f,  \"%scoreSVM)\\npred_test = modelSVM.predict(sx_test)\\nprecision_score(syw_test,pred_test,mycw)\\nrecall_score(syw_test,pred_test,mycw)\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interesting, but training takes veeeeery long\n",
    "\n",
    "# first test showed results that were similar to Random Forest\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import svm\n",
    "modelSVM = svm.SVC(kernel='linear', C=1, class_weight=mycw)\n",
    "modelSVM.fit(sx_train,np.ascontiguousarray(sy_train),sample_weight=np.ascontiguousarray(sw_train))\n",
    "scoreSVM = modelSVM.score(sx_test,sy_test,sample_weight=sw_test)\n",
    "print(\"accuracy: %f,  \"%scoreSVM)\n",
    "pred_test = modelSVM.predict(sx_test)\n",
    "precision_score(syw_test,pred_test,mycw)\n",
    "recall_score(syw_test,pred_test,mycw)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncvSVM = cross_val_score(modelSVM, x_all, y_all, cv=10)\\nprint(cvSVM)\\nprint(\"Mean accuracy: \",cvSVM.mean())\\npred_all = cross_val_predict(modelSVM, x_all, y_all, cv=10)\\nprint(\"Precision:     \", precision_score(y_all,pred_all))\\nprint(\"Recall:        \", recall_score(y_all,pred_all))\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cvSVM = cross_val_score(modelSVM, x_all, y_all, cv=10)\n",
    "print(cvSVM)\n",
    "print(\"Mean accuracy: \",cvSVM.mean())\n",
    "pred_all = cross_val_predict(modelSVM, x_all, y_all, cv=10)\n",
    "print(\"Precision:     \", precision_score(y_all,pred_all))\n",
    "print(\"Recall:        \", recall_score(y_all,pred_all))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally build a simple neural net with Keras, using tensorflow as backend (because I have tensorflow installed, so it will find and use that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 20)                300       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 384\n",
      "Trainable params: 384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 613600 samples, validate on 153400 samples\n",
      "Epoch 1/20\n",
      " - 13s - loss: 107.1380 - acc: 0.7189 - val_loss: 97.3591 - val_acc: 0.7422\n",
      "Epoch 2/20\n",
      " - 12s - loss: 97.6670 - acc: 0.7434 - val_loss: 94.4697 - val_acc: 0.7467\n",
      "Epoch 3/20\n",
      " - 12s - loss: 95.8444 - acc: 0.7457 - val_loss: 92.9195 - val_acc: 0.7518\n",
      "Epoch 4/20\n",
      " - 12s - loss: 94.6739 - acc: 0.7485 - val_loss: 91.9535 - val_acc: 0.7505\n",
      "Epoch 5/20\n",
      " - 12s - loss: 93.6803 - acc: 0.7510 - val_loss: 91.1903 - val_acc: 0.7546\n",
      "Epoch 6/20\n",
      " - 12s - loss: 93.0415 - acc: 0.7526 - val_loss: 91.1041 - val_acc: 0.7481\n",
      "Epoch 7/20\n",
      " - 12s - loss: 92.5234 - acc: 0.7543 - val_loss: 90.2814 - val_acc: 0.7577\n",
      "Epoch 8/20\n",
      " - 12s - loss: 92.0856 - acc: 0.7549 - val_loss: 91.3696 - val_acc: 0.7479\n",
      "Epoch 9/20\n",
      " - 12s - loss: 91.7552 - acc: 0.7561 - val_loss: 89.6997 - val_acc: 0.7582\n",
      "Epoch 10/20\n",
      " - 12s - loss: 91.5329 - acc: 0.7570 - val_loss: 89.6596 - val_acc: 0.7543\n",
      "Epoch 11/20\n",
      " - 13s - loss: 91.2614 - acc: 0.7571 - val_loss: 89.1729 - val_acc: 0.7598\n",
      "Epoch 12/20\n",
      " - 12s - loss: 91.0398 - acc: 0.7585 - val_loss: 89.2468 - val_acc: 0.7564\n",
      "Epoch 13/20\n",
      " - 12s - loss: 90.9002 - acc: 0.7580 - val_loss: 89.0588 - val_acc: 0.7578\n",
      "Epoch 14/20\n",
      " - 12s - loss: 90.7445 - acc: 0.7581 - val_loss: 89.3815 - val_acc: 0.7556\n",
      "Epoch 15/20\n",
      " - 13s - loss: 90.6375 - acc: 0.7587 - val_loss: 88.8226 - val_acc: 0.7574\n",
      "Epoch 16/20\n",
      " - 13s - loss: 90.3790 - acc: 0.7586 - val_loss: 88.7819 - val_acc: 0.7581\n",
      "Epoch 17/20\n",
      " - 13s - loss: 90.3037 - acc: 0.7594 - val_loss: 88.8484 - val_acc: 0.7538\n",
      "Epoch 18/20\n",
      " - 12s - loss: 90.1922 - acc: 0.7592 - val_loss: 88.5143 - val_acc: 0.7567\n",
      "Epoch 19/20\n",
      " - 12s - loss: 90.0607 - acc: 0.7595 - val_loss: 88.7973 - val_acc: 0.7607\n",
      "Epoch 20/20\n",
      " - 13s - loss: 89.9496 - acc: 0.7597 - val_loss: 88.2138 - val_acc: 0.7615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you want to try cross validation:\\n\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\n# Wrap our Keras model in an estimator compatible with scikit_learn\\nmodelNN = KerasClassifier(build_fn=create_model, nb_epoch=100, verbose=0)\\n\\ncvNN = cross_val_score(modelNN, sx_all, y_all, cv=10)\\n\\nprint(cvNN)\\nprint(\"Mean accuracy: \",cvNN.mean())\\npred_all = cross_val_predict(modelNN, sx_all, y_all, cv=10)\\nprecision_score(y_all,pred_all, mycw)\\nrecall_score(y_all,pred_all, mycw)\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "cy_all = to_categorical(y_all)\n",
    "cy_train = to_categorical(y_train)\n",
    "cy_test = to_categorical(y_test)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=14, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(12, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "create_model().summary()\n",
    "model = create_model()\n",
    "\n",
    "result = model.fit(x_train, cy_train,\n",
    "                    sample_weight = w_train,\n",
    "                    class_weight = mycw,\n",
    "                    batch_size=100,\n",
    "                    epochs=20,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, cy_test, w_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If you want to try cross validation:\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Wrap our Keras model in an estimator compatible with scikit_learn\n",
    "modelNN = KerasClassifier(build_fn=create_model, nb_epoch=100, verbose=0)\n",
    "\n",
    "cvNN = cross_val_score(modelNN, sx_all, y_all, cv=10)\n",
    "\n",
    "print(cvNN)\n",
    "print(\"Mean accuracy: \",cvNN.mean())\n",
    "pred_all = cross_val_predict(modelNN, sx_all, y_all, cv=10)\n",
    "precision_score(y_all,pred_all, mycw)\n",
    "recall_score(y_all,pred_all, mycw)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class:  [2.372495198600872e-05, 3.2794758088046275e-05, 0.5613527018967451, 0.9146700631380567]\n",
      "Precision average:    0.36901982118621895\n",
      "Precision gammas:     2.695160534281574e-05\n",
      "Precision hadrons:    0.9132292806761293\n",
      "Recall per class:     [0.7867186223170125, 0.5833222614789172, 0.018119898125537326, 0.7475535478679264]\n",
      "Recall average:       0.5339285824473483\n",
      "Recall gammas:        0.6835435217888969\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(x_test)\n",
    "fp = np.argmax(prediction, axis=1)\n",
    "precision_score(syw_test,fp, mycw)\n",
    "recall_score(syw_test,fp, mycw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "- Great, a better result than Random Forest, with very \"standard choices\"    \n",
    "- We have 68% gamma passing rate, but with the same network setup I have seen up to ~75%, so maybe sometimes the sample is biased and we get quite different results...? Needs more exploring, but always is as good or better than a random forest.\n",
    "- We get a ~91% hadron precision, i.e. rejection of background.\n",
    "- Probably could have stopped after fewer than 20 epochs without much worse results, judging by the accuracy development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Layers\n",
    "Let's try it with more hidden layers and some dropout..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 16)                240       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 412\n",
      "Trainable params: 412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 613600 samples, validate on 153400 samples\n",
      "Epoch 1/20\n",
      " - 15s - loss: 121.2048 - acc: 0.6552 - val_loss: 106.3550 - val_acc: 0.6911\n",
      "Epoch 2/20\n",
      " - 14s - loss: 106.9922 - acc: 0.6981 - val_loss: 96.4622 - val_acc: 0.7509\n",
      "Epoch 3/20\n",
      " - 14s - loss: 103.5785 - acc: 0.7152 - val_loss: 95.4145 - val_acc: 0.7550\n",
      "Epoch 4/20\n",
      " - 14s - loss: 102.4466 - acc: 0.7164 - val_loss: 94.2488 - val_acc: 0.7557\n",
      "Epoch 5/20\n",
      " - 14s - loss: 101.7228 - acc: 0.7181 - val_loss: 93.3342 - val_acc: 0.7563\n",
      "Epoch 6/20\n",
      " - 14s - loss: 101.0010 - acc: 0.7191 - val_loss: 92.8124 - val_acc: 0.7544\n",
      "Epoch 7/20\n"
     ]
    }
   ],
   "source": [
    "def multilayer_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=14, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "multilayer_model().summary()\n",
    "MLmodel = multilayer_model()\n",
    "\n",
    "result = MLmodel.fit(x_train, cy_train,\n",
    "                    sample_weight = w_train,\n",
    "                    class_weight = mycw,\n",
    "                    batch_size=100,\n",
    "                    epochs=20,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, cy_test, w_test))\n",
    "\n",
    "prediction = MLmodel.predict(x_test)\n",
    "fp = np.argmax(prediction, axis=1)\n",
    "precision_score(syw_test,fp, mycw)\n",
    "recall_score(syw_test,fp, mycw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "Hmm, more hidden layers actually lower the gamma passing rate and don't improve the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
